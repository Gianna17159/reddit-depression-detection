{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gianna17159/reddit-depression-detection/blob/main/Reddit_Depression_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jFvbbC6VtZm"
      },
      "source": [
        "# Reddit Depression Final Project\n",
        "Link to the paper: https://dl.acm.org/doi/pdf/10.1145/3578503.3583621\n",
        "\n",
        "Read through the paper fully before starting the assignment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoBxKQ_OVl-j",
        "outputId": "bc1b833f-72d2-42ec-d261-c73451132810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_validate, cross_val_score, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "!pip install pandas==2.2.2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "FOLDER = \"/content/drive/MyDrive/cs1460/fp1\"\n",
        "FILEPATH = f\"{FOLDER}/student.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz0QZLTO3xMv",
        "outputId": "5dbfae86-3ec5-44b3-c253-b39cca3997e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting happiestfuntokenizing\n",
            "  Downloading happiestfuntokenizing-0.0.7.tar.gz (6.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: happiestfuntokenizing\n",
            "  Building wheel for happiestfuntokenizing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for happiestfuntokenizing: filename=happiestfuntokenizing-0.0.7-py3-none-any.whl size=6711 sha256=546f11d0214b7de8dbea0ac2a3fea70271d8cb76685e397a9e81b596fb302287\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/c9/4d/310f0c60855eb7b428558f29d93cf464dbb64c1b8628753395\n",
            "Successfully built happiestfuntokenizing\n",
            "Installing collected packages: happiestfuntokenizing\n",
            "Successfully installed happiestfuntokenizing-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install happiestfuntokenizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcMOTL7mV9T9"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icnzto8GWFlb"
      },
      "outputs": [],
      "source": [
        "def load():\n",
        "  \"\"\"Load pickles\"\"\"\n",
        "  with open(FILEPATH, 'rb') as f:\n",
        "    data = pd.read_pickle(f)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohOK3wCdWpnA"
      },
      "outputs": [],
      "source": [
        "# List of depression subreddits in the paper\n",
        "depression_subreddits = [\"Anger\",\n",
        "    \"anhedonia\", \"DeadBedrooms\",\n",
        "    \"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\",\n",
        "    \"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\",\n",
        "    \"ForeverAlone\", \"lonely\",\n",
        "    \"cry\", \"grief\", \"sad\", \"Sadness\",\n",
        "    \"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\",\n",
        "    \"insomnia\", \"sleep\",\n",
        "    \"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\",\n",
        "    \"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"\n",
        "]\n",
        "anger_subreddits = [\"Anger\"]\n",
        "anhedonia_subreddits = [\"anhedonia\", \"DeadBedrooms\"]\n",
        "anxiety_subreddits = [\"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\"]\n",
        "eating_subreddits = [\"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\"]\n",
        "loneliness_subreddits = [\"ForeverAlone\", \"lonely\"]\n",
        "sadness_subreddits = [\"cry\", \"grief\", \"sad\", \"Sadness\"]\n",
        "loathing_subreddits = [\"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\"]\n",
        "insomnia_subreddits = [\"insomnia\", \"sleep\"]\n",
        "somatic_subreddits = [\"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\"]\n",
        "worthless_subreddits = [\"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"]\n",
        "\n",
        "#symptoms represented by integers for dataset\n",
        "symptoms = [\"control\",        #0\n",
        "            \"anger\",          #1\n",
        "            \"anhedonia\",      #2\n",
        "            \"anxiety\",        #3\n",
        "            \"eating\",         #4\n",
        "            \"loneliness\",     #5\n",
        "            \"sadness\",        #6\n",
        "            \"loathing\",       #7\n",
        "            \"insomnia\",       #8\n",
        "            \"somatic\",        #9\n",
        "            \"worthlessness\"]  #10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_RJ8pkw4t1T"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def dataset_generation(data):\n",
        "  \"\"\"Build control and symptom datasets\"\"\"\n",
        "  \"\"\"\n",
        "  Parameters: data -- a pandas dataframe\n",
        "\n",
        "  Filters through original dataset to build control and symptom datasets. All\n",
        "  final datasets are saved as pickle files.\n",
        "  \"\"\"\n",
        "\n",
        "  control_dict = {\"text\": [], \"label\": []}\n",
        "  depression_dict = {\"text\": [], \"label\": []}\n",
        "\n",
        "  pos_control_dict = {\"text\": [], \"author\": [], \"created_utc\": []}\n",
        "  authors_dict = {\"author\": [], \"earliest date\": []}\n",
        "\n",
        "  #iterate through original data\n",
        "  for row in tqdm(data.itertuples(),  total=data.shape[0], desc=f'Reading DF'):\n",
        "\n",
        "    #update depression and symptom dicts if subreddit is in depression_subreddits\n",
        "    if row.subreddit in depression_subreddits:\n",
        "\n",
        "      #find symptom idx\n",
        "      if row.subreddit in anger_subreddits:\n",
        "        idx = 1\n",
        "      elif row.subreddit in anhedonia_subreddits:\n",
        "        idx = 2\n",
        "      elif row.subreddit in anxiety_subreddits:\n",
        "        idx = 3\n",
        "      elif row.subreddit in eating_subreddits:\n",
        "        idx = 4\n",
        "      elif row.subreddit in loneliness_subreddits:\n",
        "        idx = 5\n",
        "      elif row.subreddit in sadness_subreddits:\n",
        "        idx = 6\n",
        "      elif row.subreddit in loathing_subreddits:\n",
        "        idx = 7\n",
        "      elif row.subreddit in insomnia_subreddits:\n",
        "        idx = 8\n",
        "      elif row.subreddit in somatic_subreddits:\n",
        "        idx = 9\n",
        "      elif row.subreddit in worthless_subreddits:\n",
        "        idx = 10\n",
        "\n",
        "      #update depression and all data dicts\n",
        "      depression_dict[\"text\"].append(row.text)\n",
        "      depression_dict[\"label\"].append(idx)\n",
        "\n",
        "      #add new author and their post to author_dict\n",
        "      if row.author not in authors_dict[\"author\"]:\n",
        "        authors_dict[\"author\"].append(row.author)\n",
        "        authors_dict[\"earliest date\"].append(row.created_utc)\n",
        "      else:\n",
        "        #update earliest post utc if current post is earlier\n",
        "        if authors_dict[\"earliest date\"][authors_dict[\"author\"].index(row.author)] > row.created_utc:\n",
        "          authors_dict[\"earliest date\"][authors_dict[\"author\"].index(row.author)] = row.created_utc\n",
        "    else:\n",
        "      #add all non-depression posts to possible control dictionary\n",
        "      pos_control_dict[\"text\"].append(row.text)\n",
        "      pos_control_dict[\"author\"].append(row.author)\n",
        "      pos_control_dict[\"created_utc\"].append(row.created_utc)\n",
        "\n",
        "  #create df from possible controls, filter using author list, and iterate through them\n",
        "  pos_control_df = pd.DataFrame(pos_control_dict)\n",
        "  pos_control_df = pos_control_df.loc[pos_control_df[\"author\"].isin(authors_dict[\"author\"])]\n",
        "\n",
        "  for row in tqdm(pos_control_df.itertuples(),  total=pos_control_df.shape[0], desc=f'Reading PCDF'):\n",
        "\n",
        "    #if possible control post is at least 180 days older than earliest depression post\n",
        "    #by same author, add post to final control dataset\n",
        "    earliest_timestamp = authors_dict[\"earliest date\"][authors_dict[\"author\"].index(row.author)]\n",
        "    earliest_date = datetime.fromtimestamp(earliest_timestamp)\n",
        "    cur_date = datetime.fromtimestamp(row.created_utc)\n",
        "    difference = earliest_date - cur_date\n",
        "    if difference.days >= 180:\n",
        "      control_dict[\"text\"].append(row.text)\n",
        "      control_dict[\"label\"].append(0)\n",
        "\n",
        "  #create pickle files for all dataframes\n",
        "  pickle.dump(depression_dict, open(f\"{FOLDER}/depression_dict.pkl\", \"wb\"))\n",
        "  pickle.dump(control_dict, open(f\"{FOLDER}/control_dict.pkl\", \"wb\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWGVUju_WxuP"
      },
      "outputs": [],
      "source": [
        "from happiestfuntokenizing.happiestfuntokenizing import Tokenizer\n",
        "import string\n",
        "\n",
        "def tokenize(posts : dict) -> dict:\n",
        "  \"\"\"Tokenize\"\"\"\n",
        "  tokenizer = Tokenizer(preserve_case=False)\n",
        "  tokenized_posts = []\n",
        "  for post in posts[\"text\"]:\n",
        "\n",
        "    #removing punctuation\n",
        "    post = post.translate(str.maketrans('', '', string.punctuation))\n",
        "    #tokenize, add tokenized post to post list\n",
        "    tokenized_post = tokenizer.tokenize(post)\n",
        "    tokenized_posts.append(tokenized_post)\n",
        "  #update text column to be tokenized posts\n",
        "  posts[\"text\"] = tokenized_posts\n",
        "  return posts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3j9z7UuW3eG"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def stop_words(control: dict) -> list[str]:\n",
        "  \"\"\"\n",
        "  Find top 100 words from Reddit control dataset to use as stop words\n",
        "  parameter: control -- list of tokenized control posts\n",
        "  returns: stop_words -- list of top 100 words from control dataset\n",
        "  \"\"\"\n",
        "  words = Counter()\n",
        "  for post in control[\"text\"]:\n",
        "    for word in post:\n",
        "      words[word] += 1\n",
        "  stop_words = words.most_common(100)\n",
        "  stop_words = [word[0] for word in stop_words]\n",
        "  return stop_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYhRv9fEIuAj"
      },
      "outputs": [],
      "source": [
        "def remove_stop_words(all_posts: dict, stop_words: list[str]) -> list[list[str]]:\n",
        "  \"\"\"\n",
        "  remove stop words from all posts\n",
        "  parameter: all_posts -- list of tokenized posts\n",
        "             stop_words -- list of stop words\n",
        "  returns: all_posts -- list of tokenized posts with stop words removed\n",
        "  \"\"\"\n",
        "  for post in all_posts[\"text\"]:\n",
        "    for word in post:\n",
        "      if word in stop_words:\n",
        "        post.remove(word)\n",
        "  return all_posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMXTEj3g7qx3"
      },
      "outputs": [],
      "source": [
        "data = load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhKktFNK9tsV"
      },
      "outputs": [],
      "source": [
        "#ONLY RUN ONCE\n",
        "#apply dataset generation and preprocessing steps\n",
        "\n",
        "dataset_generation(data)\n",
        "control_tokenized = tokenize(pickle.load(open(f\"{FOLDER}/control_dict.pkl\", \"rb\")))\n",
        "depression_tokenized = tokenize(pickle.load(open(f\"{FOLDER}/depression_dict.pkl\", \"rb\")))\n",
        "stop_words = stop_words(control_tokenized)\n",
        "control_preprocessed = remove_stop_words(control_tokenized, stop_words)\n",
        "depression_preprocessed = remove_stop_words(depression_tokenized, stop_words)\n",
        "pickle.dump(control_preprocessed, open(f\"{FOLDER}/control_preprocessed.pkl\", \"wb\"))\n",
        "pickle.dump(depression_preprocessed, open(f\"{FOLDER}/depression_preprocessed.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4I37U1SXAEZ"
      },
      "source": [
        "## Reddit Topics with LDA\n",
        "\n",
        " - Don't use MALLET (as the paper does), use some other LDA implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xf3surfWXH-q"
      },
      "outputs": [],
      "source": [
        "# We highly recommend you using the LdaMulticore interface, but feel free to use any other implementations if you prefer.\n",
        "# from gensim.models import LdaMulticore\n",
        "\n",
        "# TODO: Your LDA code!\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "\n",
        "def train_topic_model(corpus: np.ndarray, id2word: dict, num_topics: int) -> LdaMulticore:\n",
        "  \"\"\"\n",
        "  Train a topic model on a corpus of posts.\n",
        "\n",
        "  parameter: corpus -- term-document matrix\n",
        "             id2word -- dictionary mapping word indices to words\n",
        "             num_topics -- number of topics to train the model with\n",
        "  returns: model -- trained LDA model\n",
        "  \"\"\"\n",
        "  model = LdaMulticore(corpus, id2word=id2word, num_topics=num_topics, passes=3, random_state=42)\n",
        "  return model\n",
        "\n",
        "\n",
        "def get_topic_probabilities(model, corpus)-> list[float]:\n",
        "  \"\"\"\n",
        "  Get the topic probabilities for each post in the corpus.\n",
        "\n",
        "  parameter: model -- trained LDA model\n",
        "             corpus -- term-document matrix\n",
        "  returns: topic_probabilities -- list of topic probabilities for each post\n",
        "  \"\"\"\n",
        "  topic_probabilities = []\n",
        "  for doc in corpus:\n",
        "    doc_topics = model.get_document_topics(doc)\n",
        "    topic_probs = [0] * model.num_topics\n",
        "    for topic, prob in doc_topics:\n",
        "      topic_probs[topic] = prob\n",
        "    topic_probabilities.append(topic_probs)\n",
        "  return topic_probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFCgKP7K4cKe"
      },
      "outputs": [],
      "source": [
        "#load datasets\n",
        "control_preprocessed = pickle.load(open(f\"{FOLDER}/control_preprocessed.pkl\", \"rb\"))\n",
        "depression_preprocessed = pickle.load(open(f\"{FOLDER}/depression_preprocessed.pkl\", \"rb\"))\n",
        "\n",
        "#recombine control and depression datasets\n",
        "all_posts = {\"text\": control_preprocessed[\"text\"] + depression_preprocessed[\"text\"],\n",
        "             \"label\": control_preprocessed[\"label\"] + depression_preprocessed[\"label\"]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7stHilxgA3ll"
      },
      "outputs": [],
      "source": [
        "#ONLY RUN ONCE\n",
        "#create dictionary and vector representations of posts, run LDA training, takes ~22 minutes\n",
        "dictionary = Dictionary(all_posts[\"text\"])\n",
        "corpus = [dictionary.doc2bow(text) for text in all_posts[\"text\"]]\n",
        "lda_model = train_topic_model(corpus=corpus,id2word=dictionary, num_topics=200)\n",
        "\n",
        "#calculate topic probability for each post, takes ~4-5 minutes\n",
        "topic_probabilities = get_topic_probabilities(lda_model, corpus)\n",
        "\n",
        "#add topic probabilities for each post to df\n",
        "all_posts_df = pd.DataFrame(all_posts)\n",
        "all_posts_df[\"topic_probabilities\"] = topic_probabilities\n",
        "\n",
        "#pickle, only run once\n",
        "pickle.dump(all_posts_df, open(f\"{FOLDER}/after_lda_df.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0-97hsVXNkF"
      },
      "source": [
        "## RoBERTa Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blx1SWVMXYDp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#ONLY RUN ONCE\n",
        "\n",
        "from transformers import AutoConfig, AutoModelForMaskedLM, AutoTokenizer\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Load model directly\n",
        "config = AutoConfig.from_pretrained(\"distilroberta-base\")\n",
        "config.output_hidden_states = True\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\", add_prefix_space=True)\n",
        "roberta_model = AutoModelForMaskedLM.from_pretrained(\"distilroberta-base\", config=config)\n",
        "\n",
        "\n",
        "def get_roberta_embeddings(model, post, tokenizer) -> list[float]:\n",
        "  \"\"\"\n",
        "  Uses pretrained model to get embedding representation of each post in corpus.\n",
        "  parameters - model: pretrained model, tokenizer: tokenizer for post encoding,\n",
        "                posts: tokenized posts\n",
        "  returns - list of embeddings for each post\n",
        "  \"\"\"\n",
        "  model.to('cuda')\n",
        "  model.eval()\n",
        "\n",
        "  #represent empty posts, which previously exclusively contained stop words\n",
        "  if not post:\n",
        "    post = [\"\"]\n",
        "\n",
        "  #tokenize, return tensors\n",
        "  input = tokenizer(post, padding=True, max_length=100, truncation=True,\n",
        "                    is_split_into_words=True, return_tensors='pt')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    #put input to gpu, find outputs\n",
        "    input.to('cuda')\n",
        "    output = model(**input)\n",
        "    #memory saving measures\n",
        "    input.to('cpu')\n",
        "    del input\n",
        "    torch.cuda.empty_cache()\n",
        "    #access output of 5th layer, which is index 5 when accounting for word embedding output\n",
        "    #find average of token embeddings for post embedding\n",
        "    post_embeddings = torch.mean(output.hidden_states[5].squeeze().cpu(), dim=0)\n",
        "  return post_embeddings\n",
        "\n",
        "\n",
        "\n",
        "#get roberta embeddings (~14-15 minutes) for all posts and add to all_posts dataframe\n",
        "all_posts_df = pickle.load(open(f\"{FOLDER}/after_lda_df.pkl\", \"rb\"))\n",
        "\n",
        "post_embeddings = [get_roberta_embeddings(roberta_model, all_posts_df[\"text\"][i], tokenizer)\n",
        "                  for i in tqdm(range(len(all_posts_df[\"text\"])), desc=f'Getting post embeddings')]\n",
        "\n",
        "all_posts_df[\"roberta_embeddings\"] = post_embeddings\n",
        "pickle.dump(all_posts_df, open(f\"{FOLDER}/after_roberta_df.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDWxuF2jXtwi"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koTBPhcDXujb"
      },
      "outputs": [],
      "source": [
        "def main(X, y):\n",
        "  \"\"\"\n",
        "  Here's the basic structure of the main block! It should run\n",
        "  5-fold cross validation with random forest to evaluate your RoBERTa and LDA\n",
        "  performance.\n",
        "  \"\"\"\n",
        "  rf_classifier = RandomForestClassifier()\n",
        "  cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "  results = cross_validate(rf_classifier, X=X, y=y, cv=cv, scoring='roc_auc', return_train_score=True)\n",
        "\n",
        "  # TODO: Print your training and testing scores!\n",
        "  avg_train_score = np.mean(results['train_score'])\n",
        "  avg_test_score = np.mean(results['test_score'])\n",
        "  print(f\"Training score: {avg_train_score}\")\n",
        "  print(f\"Testing score: {avg_test_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMdC9vYZtEH-",
        "outputId": "aee36ed3-4963-45d6-adb0-8229e4b9f65b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anger LDA\n",
            "Training score: 0.9992857174409266\n",
            "Testing score: 0.9346233191775315\n",
            "Anhedonia LDA\n",
            "Training score: 0.9995597758628092\n",
            "Testing score: 0.9598393680870757\n",
            "Anxiety LDA\n",
            "Training score: 0.9998807385164881\n",
            "Testing score: 0.9401399206229929\n",
            "Eating LDA\n",
            "Training score: 0.9993281109981856\n",
            "Testing score: 0.9586837041012789\n",
            "Loneliness LDA\n",
            "Training score: 0.9998413671993003\n",
            "Testing score: 0.8647183865958381\n",
            "Sadness LDA\n",
            "Training score: 0.9994498659547606\n",
            "Testing score: 0.8527168959859776\n",
            "Loathing LDA\n",
            "Training score: 0.9993851983552184\n",
            "Testing score: 0.8819954616000654\n",
            "Insomnia LDA\n",
            "Training score: 0.9996680930717463\n",
            "Testing score: 0.9782681950346429\n",
            "Somatic LDA\n",
            "Training score: 0.9993919133526784\n",
            "Testing score: 0.9193625251720301\n",
            "Worthless LDA\n",
            "Training score: 0.9981798009859274\n",
            "Testing score: 0.657632803439651\n"
          ]
        }
      ],
      "source": [
        "#create symptom dataframes, perform both lda and roberta testing for each symptom\n",
        "from sklearn.model_selection import train_test_split\n",
        "all_posts_df = pickle.load(open(f\"{FOLDER}/after_roberta_df.pkl\", \"rb\"))\n",
        "\n",
        "#symptom datasets with control added for one to one classification\n",
        "#symptom datasets with control added for one to one classification\n",
        "anger_df = all_posts_df.loc[(all_posts_df[\"label\"] == 1) | (all_posts_df[\"label\"] == 0)]\n",
        "anhedonia_df = all_posts_df.loc[(all_posts_df[\"label\"] == 2) | (all_posts_df[\"label\"] == 0)]\n",
        "anxiety_df = all_posts_df.loc[(all_posts_df[\"label\"] == 3) | (all_posts_df[\"label\"] == 0)]\n",
        "eating_df = all_posts_df.loc[(all_posts_df[\"label\"] == 4) | (all_posts_df[\"label\"] == 0)]\n",
        "loneliness_df = all_posts_df.loc[(all_posts_df[\"label\"] == 5) | (all_posts_df[\"label\"] == 0)]\n",
        "sadness_df = all_posts_df.loc[(all_posts_df[\"label\"] == 6) | (all_posts_df[\"label\"] == 0)]\n",
        "loathing_df = all_posts_df.loc[(all_posts_df[\"label\"] == 7) | (all_posts_df[\"label\"] == 0)]\n",
        "insomnia_df = all_posts_df.loc[(all_posts_df[\"label\"] == 8) | (all_posts_df[\"label\"] == 0)]\n",
        "somatic_df = all_posts_df.loc[(all_posts_df[\"label\"] == 9) | (all_posts_df[\"label\"] == 0)]\n",
        "worthless_df = all_posts_df.loc[(all_posts_df[\"label\"] == 10) | (all_posts_df[\"label\"] == 0)]\n",
        "\n",
        "#lda testing with topic probabilities, converted to list for classifier compatibility\n",
        "\n",
        "#anger\n",
        "print(\"Anger LDA\")\n",
        "main(anger_df[\"topic_probabilities\"].tolist(), anger_df[\"label\"].tolist())\n",
        "\n",
        "#anhedonia\n",
        "print(\"Anhedonia LDA\")\n",
        "main(anhedonia_df[\"topic_probabilities\"].tolist(), anhedonia_df[\"label\"].tolist())\n",
        "\n",
        "#anxiety\n",
        "print(\"Anxiety LDA\")\n",
        "main(anxiety_df[\"topic_probabilities\"].tolist(), anxiety_df[\"label\"].tolist())\n",
        "\n",
        "#eating\n",
        "print(\"Eating LDA\")\n",
        "main(eating_df[\"topic_probabilities\"].tolist(), eating_df[\"label\"].tolist())\n",
        "\n",
        "#loneliness\n",
        "print(\"Loneliness LDA\")\n",
        "main(loneliness_df[\"topic_probabilities\"].tolist(), loneliness_df[\"label\"].tolist())\n",
        "\n",
        "#sadness\n",
        "print(\"Sadness LDA\")\n",
        "main(sadness_df[\"topic_probabilities\"].tolist(), sadness_df[\"label\"].tolist())\n",
        "\n",
        "#loathing\n",
        "print(\"Loathing LDA\")\n",
        "main(loathing_df[\"topic_probabilities\"].tolist(), loathing_df[\"label\"].tolist())\n",
        "\n",
        "#insomnia\n",
        "print(\"Insomnia LDA\")\n",
        "main(insomnia_df[\"topic_probabilities\"].tolist(), insomnia_df[\"label\"].tolist())\n",
        "\n",
        "#somatic\n",
        "print(\"Somatic LDA\")\n",
        "main(somatic_df[\"topic_probabilities\"].tolist(), somatic_df[\"label\"].tolist())\n",
        "\n",
        "#worthless\n",
        "print(\"Worthless LDA\")\n",
        "main(worthless_df[\"topic_probabilities\"].tolist(), worthless_df[\"label\"].tolist())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3iZYYvA_e8t",
        "outputId": "a621a834-cc8d-4d0c-fbd2-f2d46a4d7439"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Anger distilRoberta\n",
            "Training score: 0.9999993566656874\n",
            "Testing score: 0.9341479404465952\n",
            "Anhedonia distilRoberta\n",
            "Training score: 0.9999994474437205\n",
            "Testing score: 0.9583828131147701\n",
            "Anxiety distilRoberta\n",
            "Training score: 0.9999746613213343\n",
            "Testing score: 0.9523284098927516\n",
            "Eating distilRoberta\n",
            "Training score: 1.0\n",
            "Testing score: 0.9657040676961428\n",
            "Loneliness distilRoberta\n",
            "Training score: 0.9999374560665926\n",
            "Testing score: 0.9077385469268918\n",
            "Sadness distilRoberta\n",
            "Training score: 0.9999018379174591\n",
            "Testing score: 0.9183706115966425\n",
            "Loathing distilRoberta\n",
            "Training score: 0.9999778148156869\n",
            "Testing score: 0.9284033092691135\n",
            "Insomnia distilRoberta\n",
            "Training score: 1.0\n",
            "Testing score: 0.9773468964051535\n",
            "Somatic distilRoberta\n",
            "Training score: 0.9999998027087538\n",
            "Testing score: 0.9392502671435782\n",
            "Worthless distilRoberta\n",
            "Training score: 1.0\n",
            "Testing score: 0.816783019663774\n"
          ]
        }
      ],
      "source": [
        "#distilRoberta testing with post embeddings\n",
        "\n",
        "#anger\n",
        "print(\"Anger distilRoberta\")\n",
        "main(anger_df[\"roberta_embeddings\"].tolist(), anger_df[\"label\"].tolist())\n",
        "\n",
        "#anhedonia\n",
        "print(\"Anhedonia distilRoberta\")\n",
        "main(anhedonia_df[\"roberta_embeddings\"].tolist(), anhedonia_df[\"label\"].tolist())\n",
        "\n",
        "#anxiety\n",
        "print(\"Anxiety distilRoberta\")\n",
        "main(anxiety_df[\"roberta_embeddings\"].tolist(), anxiety_df[\"label\"].tolist())\n",
        "\n",
        "#eating\n",
        "print(\"Eating distilRoberta\")\n",
        "main(eating_df[\"roberta_embeddings\"].tolist(), eating_df[\"label\"].tolist())\n",
        "\n",
        "#loneliness\n",
        "print(\"Loneliness distilRoberta\")\n",
        "main(loneliness_df[\"roberta_embeddings\"].tolist(), loneliness_df[\"label\"].tolist())\n",
        "\n",
        "#sadness\n",
        "print(\"Sadness distilRoberta\")\n",
        "main(sadness_df[\"roberta_embeddings\"].tolist(), sadness_df[\"label\"].tolist())\n",
        "\n",
        "#loathing\n",
        "print(\"Loathing distilRoberta\")\n",
        "main(loathing_df[\"roberta_embeddings\"].tolist(), loathing_df[\"label\"].tolist())\n",
        "\n",
        "#insomnia\n",
        "print(\"Insomnia distilRoberta\")\n",
        "main(insomnia_df[\"roberta_embeddings\"].tolist(), insomnia_df[\"label\"].tolist())\n",
        "\n",
        "#somatic\n",
        "print(\"Somatic distilRoberta\")\n",
        "main(somatic_df[\"roberta_embeddings\"].tolist(), somatic_df[\"label\"].tolist())\n",
        "\n",
        "#worthless\n",
        "print(\"Worthless distilRoberta\")\n",
        "main(worthless_df[\"roberta_embeddings\"].tolist(), worthless_df[\"label\"].tolist())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}